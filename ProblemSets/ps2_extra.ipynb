{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n",
    "student_self_assessment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2 (Extra problems)\n",
    "\n",
    "Last revised: 11-Sep-2023 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6874d6f0612340a685cc96f6c456b90b",
     "grade": false,
     "grade_id": "cell-bf9197ccd45cb935",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 7: MCMC sampling part 2; multi-dimensional distributions, acceptance ratios and auto-correlation\n",
    "*You should have solved problems 4 and 5 before doing this problem.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f45c4d6c82c756a0e96a6a529bd55c95",
     "grade": false,
     "grade_id": "cell-f66a1e93499bc542",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A challenge when doing MCMC sampling is that the collected samples can be *correlated*. This can be tested by computing the *autocorrelation function* and extracting the correlation time for a chain of samples.\n",
    "\n",
    "Say that $X$ is an array of $N$ samples numbered by the index $t$. Then $X_{+h}$ is a shifted version of $X$ with elements $X_{t+h}$. The integer $h$ is called the *lag*. Since we have a finite number of samples, the array $X_{+h}$ will be $h$ elements shorter than $X$. \n",
    "\n",
    "Furthermore, $\\bar{X}$ is the average value of $X$.\n",
    "\n",
    "We can then define the autocorrelation function $\\rho(h)$ from the list of samples. \n",
    "$$\n",
    "\\rho(h) = \\frac{\\sum_{t=0}^{N-h-1} \\left[ (X_t - \\bar{X}) (X_{t+h} - \\bar{X})\\right]}\n",
    "{\\sqrt{ \\sum_{t=0}^{N-h-1} (X_t - \\bar{X})^2 } \\sqrt{ \\sum_{t=0}^{N-h-1} (X_{t+h} - \\bar{X})^2 }}\n",
    "$$\n",
    "The summation is carried out over the subset of samples that overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* It is often observed that $\\rho(h)$ is roughly exponential so that we can define an autocorrelation time $\\tau$ according to\n",
    "$$\n",
    "\\rho(h) \\sim \\exp(-h/\\tau).\n",
    "$$\n",
    "* Try to understand what the autocorrelation time measures and why it is large (small) when the samples are correlated (not correlated). A short explanation of the autocorrelation function is found below\\*.\n",
    "\n",
    "\\* *The autocorrelation is the overlap (scalar product) of the chain of samples (the trace) with a copy of itself shifted by the lag, as a function of the lag. If the lag is short so that nearby samples are close to each other (and have not moved very far) the product of these two vectors is large. If samples are independent, you will have both positive and negative numbers in the overlap that cancel each other.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "- Repeat the MCMC sampling of a Lorentzian pdf using the random walk Metropolis algorithm from Problem 3. However, we will make the sampler numerically more stable by working with the logarithm of the probability densities rather than with the densities.\n",
    "- Create a two-dimensional sampler for which the posterior function takes two arguments so that you can apply it to the signal amplitude plus background model from Problem 4. The proposal distribution will now be a two-dimensional Gaussian. We will use the simplest possible multivariate Gaussian with a single width parameter (see `scipy.stats.multivariate_normal`). Also this sampler should work with log probability densities.\n",
    "- Re-analyze the baseline case. Rather than scanning the two-dimensional parameter space, as we did in Problem 4, you can now collect samples from the posterior pdf.\n",
    "\n",
    "Extract both the autocorrelation time $\\tau$ and the acceptance ratio $r$ for some different choices of the proposal width; e.g. 0.1, 0.5, 1.0, 2.0. What is a good choice for sampling these distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62b651978959e6c9f9aa81d5fba86bdd",
     "grade": false,
     "grade_id": "cell-5a020fc551cbfaa4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sampler algorithms\n",
    "# - 1d sampler from Problem 3 (without verbose printing). But using the log pdf.\n",
    "# - 2d sampler (without plotting) again using the log pdf.\n",
    "\n",
    "def sampler_1d(log_posterior_func, no_of_samples=4, start_position=.5, \n",
    "            proposal_width=1.):\n",
    "    '''\n",
    "    Metropolis sampler function for a one-dimensional pdf.\n",
    "    \n",
    "    Args:\n",
    "        log_posterior_func: Function that takes a single positional argument and returns \n",
    "            the logarithm of the (possibly unnormalized) pdf value.\n",
    "        no_of_samples: (integer) Number of samples that will be returned (excluding the start position). \n",
    "            (default=4)\n",
    "        start_position: (float) Start position. (default=0.5)\n",
    "        proposal_width: (float) Width of Gaussian proposal distribution. (default=1.)\n",
    "        \n",
    "    Returns:\n",
    "        samples: Array of floats. Length = no_of_samples+1\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def sampler_2d(log_posterior_func, no_of_samples=4, start_position=[.5,.5], \n",
    "            proposal_width=1.):\n",
    "    '''\n",
    "    Metropolis sampler function for a two-dimensional pdf.\n",
    "    \n",
    "    Args:\n",
    "        log_posterior_func: Function that takes two positional arguments and returns \n",
    "            the logarithm of the (possibly unnormalized) pdf value.\n",
    "        no_of_samples: (integer) Number of samples that will be returned (excluding the start position). \n",
    "            (default=4)\n",
    "        start_position: (list or array of floats) Start position. (default=[0.5,0.5])\n",
    "        proposal_width: (float) Width of symmetric Gaussian proposal distribution. (default=1.)\n",
    "        \n",
    "    Returns:\n",
    "        samples: Array of floats. samples.shape = (no_of_samples+1,2)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c812aca8e93452a2b8f6ebde62d28357",
     "grade": false,
     "grade_id": "cell-fe2e30e2209c73aa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Copy the posterior function from Problem 3 \n",
    "\n",
    "# One-dimensional problem\n",
    "#\n",
    "def log_posterior_function_1d(x, normalized=False):\n",
    "    '''\n",
    "    Return the posterior pdf given by a standard Cauchy (Lorentzian).\n",
    "    \n",
    "    Args:\n",
    "        x: position in a one-dimensional space\n",
    "        normalized: Return a normalized pdf if True (optional, default=False)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "665222a038720915fc798c7c17b82637",
     "grade": false,
     "grade_id": "cell-a12aa6244924d257",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Copy the log-posterior function from Problem 4.\n",
    "# Two-dimensional problem\n",
    "\n",
    "# The measured \"Baseline\" data has been generated already and will be loaded from a pickle file.\n",
    "# It is a dictionary with four keys corresponding to the four cases, and each value\n",
    "# corresponding to xk_pts, Dk_pts, Nk_pts, x_max, n_0\n",
    "with open(f'{DATA_DIR}/PS2_Prob4_data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    cases = data.keys()\n",
    "\n",
    "# Data for baseline case: 15 bins and maximum expection of 100 counts.\n",
    "xk_pts_baseline, Dk_pts_baseline, Nk_pts_baseline, x_max_baseline, n_0_baseline = \\\n",
    "    data['Baseline']\n",
    "\n",
    "# Define the pdfs and combine with Bayes' theorem.\n",
    "def exact_data(A, B, n_0, x_k, x_0=0., width=np.sqrt(5.)):\n",
    "    \"\"\"\n",
    "    Return the exact signal plus background.  The overall scale is n_0,\n",
    "    which is determined by how long counts are collected. \n",
    "    \"\"\"\n",
    "    return n_0 * (A * np.exp(-(x_k - x_0)**2/(2.*width**2)) + B)\n",
    "\n",
    "def log_prior(A, B):\n",
    "    \"\"\"\n",
    "    Log prior .\n",
    "    \n",
    "    We take a uniform (flat) prior with large enough\n",
    "    maximums but, more importantly, require positive values for A and B.\n",
    "    \"\"\"\n",
    "    A_max = 5.\n",
    "    B_max = 5.\n",
    "    # flat prior \n",
    "    if np.logical_and(A <= A_max, B <= B_max).all(): \n",
    "        return np.log(1/(A_max * B_max))\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "\n",
    "def log_likelihood(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log likelihood for data Nk_pts given A and B\"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "def log_posterior(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log posterior for data Nk_pts given A and B\"\"\"\n",
    "    return log_prior(A, B) + log_likelihood(A, B, xk_pts, Nk_pts, n_0)\n",
    "\n",
    "# \n",
    "\n",
    "def log_posterior_2d(theta):\n",
    "    '''\n",
    "    This function returns the log posterior for the specific baseline data set loaded above.\n",
    "    \n",
    "    It represents the posterior pdf that we will sample from.\n",
    "    \n",
    "    Args:\n",
    "        theta: parameter vector (array or list of floats) of length 2.\n",
    "    '''\n",
    "    return log_posterior(theta[0], theta[1], xk_pts_baseline, Nk_pts_baseline, n_0_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "069f55107e8b2c2c4f12c428a6d942cd",
     "grade": false,
     "grade_id": "cell-b6591c0b14ce1bdd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def acceptance_ratio(chain):\n",
    "    '''\n",
    "    Returns the acceptance ratio for a MCMC chain\n",
    "    \n",
    "    Args:\n",
    "        chain: 1d- or 2d-array with MCMC samples. The length of axis-0 \n",
    "            corresponds to the number of samples and the length of axis-1\n",
    "            to the number of parameters\n",
    "            \n",
    "    Returns:\n",
    "        r: Acceptance ratio (float). Note that 0 <= r <= 1.0\n",
    "            We define r = number of accepted proposed steps / # proposed steps\n",
    "            where we note that the # propsed steps = # samples - 1\n",
    "    '''\n",
    "    \n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11327e0602bbc2aae6e010a6ba4e6934",
     "grade": true,
     "grade_id": "cell-f4ffc9f497001b92",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# In this one-dimensional chain we have accepted two proposed steps out of four (the first one is the starting point)\n",
    "chain_1d = np.array([0., 0., 1., 2., 2.])\n",
    "assert np.abs( acceptance_ratio(chain_1d) - 2/( len(chain_1d)-1 )) < 0.0001\n",
    "# In this two-dimensional chain we have accepted two proposed steps out of three (the first one is the starting point)\n",
    "chain_2d = np.array([[0., 0.], [1., 2.], [1., 2.], [2., 2.]])\n",
    "assert np.abs( acceptance_ratio(chain_2d) - 2/( len(chain_2d)-1 )) < 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f7c7d8c5a18743ca21822d45668e83c",
     "grade": false,
     "grade_id": "cell-e56a5f81911123c9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def autocorrelation(chain, max_lag):\n",
    "    \"\"\"\n",
    "    Autocorrelation function rho(h) for a MCMC chain.\n",
    "    \n",
    "    Args:\n",
    "        chain: 1d- or 2d-array with MCMC samples. The length of axis-0 \n",
    "            corresponds to the number of samples, n_samples, and the length of axis-1\n",
    "            to the number of parameters, ndim.\n",
    "        max_lag: The maximum lag, h. (integer)\n",
    "        \n",
    "    Returns:\n",
    "        acors: ndarray containing the autocorrelations rho(h) for each\n",
    "                dimension of the chain separately.\n",
    "\n",
    "    The shape of the returned array is\n",
    "        -> (max_lag+1, ndim) if the shape of chain is (n_samples, ndim)\n",
    "        -> (max_lag+1,) if the shape of the chain is (n_samples,).\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53c6b6d629a6afd87af25a11d69eece0",
     "grade": true,
     "grade_id": "cell-45ea2bf4cb6dbd6d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "# Create a correlated one-dimensional chain of samples.\n",
    "chain = np.exp(-np.arange(100)/10)\n",
    "max_lag=10\n",
    "acor = autocorrelation(chain, max_lag)\n",
    "# Zero lag => max correlation\n",
    "assert acor[0]==1.\n",
    "assert np.abs( acor[-1]-0.75719323 ) < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "690a5050de0338b5a408c03643e8aa9d",
     "grade": false,
     "grade_id": "cell-1b2e8b7fd61795f3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Use four different proposal widths (as indicated below) and extract the acceptance ratio and the correlation time(s) for the one- and two-dimensional MCMC samplers. Use 10,000 samples for each sampling.\n",
    "\n",
    "Print also the median values of the sampled parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23c56784da424e29bc5a1bd5370a405f",
     "grade": false,
     "grade_id": "cell-d69562d5f723af5a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('One-dimensional MCMC sampling\\n')\n",
    "print('width   acc. ratio   tau   <x>')\n",
    "no_of_samples=10000\n",
    "\n",
    "for width in [0.1, 0.5, 1.0, 2.0]:\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc5cf97d417b63014d3ba356418142a3",
     "grade": false,
     "grade_id": "cell-7750c343138d502f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Two-dimensional MCMC sampling\\n')\n",
    "print('width   acc. ratio   tau[A]   tau[B]    <A>     <B>')\n",
    "no_of_samples=10000\n",
    "\n",
    "for width in [0.01, 0.05, 0.1, 0.5]:\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "    print(f' {width:.2f}      {acc_ratio:.2f}        {tau[0]:4d}     {tau[1]:4d}',\\\n",
    "         f'   {np.median(samples[:,0]):4.2f}   {np.median(samples[:,1]):4.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 8: Good-data, bad-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's start by defining some data that we will fit with a straight line.  The following data is measured velocities and distances for a set of galaxies. We will assume that there is an observational error on the $y$ values (and no error on $x$). \n",
    "- These errors are considered as independent and identically distributed (i.i.d.) from a normal distribution with mean 0.0 and standard deviation $\\sigma = 200$ km/sec.\n",
    "- However, as seen in the data table below, it turns out that two of the measurements resulted in \"bad data\" with errors that are much larger than what could be expected from the assumed error model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "518519da09df1054579ca39c028998cf",
     "grade": false,
     "grade_id": "cell-0579ba473ff606b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Data from student lab observations; \n",
    "# x = Galaxy distances in MPc\n",
    "# y = Galaxy velocity in km/sec\n",
    "x = np.array([10.1 ,45.2, 19.7, 31.2, 31.9, 44.0,\n",
    "       14.9, 35.1,  39.9  ])\n",
    "y = np.array([1507.9, 2937.5,  930.4, 2037.1, 2131.1,\n",
    "       2795.6, 1061.8, 2464.8, \n",
    "       1971.1])\n",
    "# Two of these samples, the 10.1 and 39.9 data points, were added by hand.\n",
    "# They are rather extreme samples from a Cauchy distribution with the same FWHM and \n",
    "# could be assumed to come from an experiment where some systematic error was missing from the analysis\n",
    "# (here leading the experimentalists to assign a Gaussian error when in fact it should have been Lorenzian).\n",
    "\n",
    "intercept = 0.\n",
    "slope = 70\n",
    "theta_true = [intercept, slope]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba4fed76f8f193381a7314210ba4ae58",
     "grade": false,
     "grade_id": "cell-b70037b8cfce4773",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data with the assumed error model indicated by a standard deviation\n",
    "dy=200\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.errorbar(x, y, dy, fmt='o')\n",
    "ax.set_xlabel(r'distance [MPc]')\n",
    "ax.set_ylabel(r'velocity [km/sec]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task (a)\n",
    "\n",
    "The question that we will be asking is: \n",
    "> What value would you infer for the Hubble constant, i.e. the slope of the velocity versus distance relation, given this data?\n",
    "\n",
    "We will make the prior assumption that the data can be fitted with a straight line. But we note that we are actually not interested in the offset of the straight line, but just its slope.\n",
    "\n",
    "We will try three different approaches:\n",
    "1. Maximum likelihood estimate\n",
    "1. Bayesian analysis\n",
    "1. Bayesian analysis incorporating a Bayesian approach to identify \"good-data-bad-data\"\n",
    "\n",
    "As a second task, we will also explore how the posterior belief from this analysis can feed into a subsequent data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical model\n",
    "Here we are given data with simple error bars (constant $\\sigma$), which implies that the probability for any *single* data point is a normal distribution about the true value. That is,\n",
    "\n",
    "$$\n",
    "y_i \\sim \\mathcal{N}(y_M(x_i;\\theta), \\sigma),\n",
    "$$\n",
    "\n",
    "with $y_M(x) = mx + b$.\n",
    "Or, in other words,\n",
    "\n",
    "$$\n",
    "p(y_i\\mid\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(\\frac{-\\left[y_i - y_M(x_i;\\theta)\\right]^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the (known) measurement error indicated by the error bars.\n",
    "\n",
    "ps Note that the assignment of normally distributed experimental errors is incorrect for two of the data points, but we don't know that (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Maximum likelihood estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a maximum likelihood estimate of the slope and the intercept. \n",
    "- Use `optimize.minimize` from `scipy` to find the maximum of the log likelihood.\n",
    "- Estimate an uncertainty by extracting the inverse of the Hessian (second derivative of the log likelihood) at the optimum. Note that the result from `optimize.minimize` will have a property `hess_inv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ba64dea91508247a4eba1ed64c00373",
     "grade": false,
     "grade_id": "cell-7af0cbb18f6b9a76",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Maximum likelihood estimate (MLE)\n",
    "#\n",
    "# The parameter vector theta = [intercept, slope]\n",
    "# Assign the two arrays with the MLE estimate and the diagonal elements of the inverse hessian\n",
    "theta_MLE = np.array([0.,0.])\n",
    "err_theta_MLE = np.array([0.,0.])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6002de29c5842296b66c33a788e0d45c",
     "grade": true,
     "grade_id": "cell-b472548bd7e1c7c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert not (theta_MLE==0).all(), f'The theta_MLE array should contain the MLE estimate: {theta_MLE}'\n",
    "assert not (err_theta_MLE==0).all(), 'The err_theta_MLE array should contain the MLE error estimate'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03c8613878e119fb95404eba1da6c729",
     "grade": false,
     "grade_id": "cell-e10d1fd9547787ad",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### Step 2: Bayesian analysis\n",
    "For the Bayesian analysis we would recommend a Gaussian prior (mean=0, standard deviation=$\\sigma = 200$ km/sec) for the intercept, and a symmetric prior\n",
    "$$\n",
    "p(m|I) \\propto \\frac{1}{(1+m^2)^{3/2}}\n",
    "$$\n",
    "for the slope (the latter was used also in the fitting straight line example).\n",
    "\n",
    "- Check the trace of the MCMC sampling to make sure (visually) that you use a proper warmup period. \n",
    "- Plot the samples using the `corner` package and extract the median and 68% credibility region for the slope parameter using the marginalized distribution.\n",
    "- Use the parameter samples to create corresponding samples of our model predictions. Show the model prediction together with the data by plotting (i) the prediction using the median, point estimate for the parameters as well as (ii) a filled band corresponding to the  68% credibility region of these sampled predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b939882818ebe4429deb3fe8f145bc3",
     "grade": false,
     "grade_id": "cell-63e104271f477475",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the log prior and the log posterior\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03f05033236bfe822fdb164c560a7a06",
     "grade": false,
     "grade_id": "cell-660d28a0fb16e9d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform MCMC sampling using emcee\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60dbfe547e908dd0c2fafbf8ab3869ff",
     "grade": false,
     "grade_id": "cell-c70e3a91a0d3acb9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Our choice of starting points were not optimal. It takes some time for the MCMC chains \n",
    "# to converge. We recommend to study the traces.\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "006c01f5d096a4799897735c96932958",
     "grade": false,
     "grade_id": "cell-6977cb3865029f17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# And choose an appropriate warm-up time\n",
    "nwarmup = 0 # update this!\n",
    "\n",
    "# we'll throw-out the warmup points, collect the chains from all walkers, \n",
    "# and reshape into an array 'samples' of shape (nsamples, 2)\n",
    "samples=np.array([])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63bac761167166954f744297948bca17",
     "grade": false,
     "grade_id": "cell-5194cb532bd5d98d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the samples using the `corner` package and extract the median and \n",
    "# 68% confidence region for the slope parameter\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e64dc95c7c8a0faf9a2b13176cbd698d",
     "grade": false,
     "grade_id": "cell-2eaf1954728a0dcb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can use the parameter samples to create corrsponding samples of our model predictions. \n",
    "# Plot the prediction using the median, point estimate for the parameters \n",
    "# as well as a filled band corresponding to the  68% credibility region of these sampled predictions.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "26e5312fe0521177030f1e3e1bf12ee9",
     "grade": false,
     "grade_id": "cell-379fd70f770d21ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Step 3: Bayesian Approach to Outliers (good-data/bad-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ed2b687b8b70bb241251e8d0e65b53e",
     "grade": false,
     "grade_id": "cell-0bb194bb977f50cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are several Bayesian approaches to accounting for outliers. They generally involve *modifying the statistical model*. For this data, it is abundantly clear that a simple straight line is not a good fit to our data. So let's propose a more complicated model that has the flexibility to account for outliers where the experimental error bar might have been underestimated. \n",
    "\n",
    "Specifically we will assume that there is a chance that the error distribution is represented by a Lorentzian distribution rather than a Gaussian. The peaks of these two distributions are very similar in shape, but the Lorentzian has much heavier tails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "915cc6c2f0f7790948325b89c94a6153",
     "grade": false,
     "grade_id": "cell-51ed6c1361de4d8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will use a statistical model in which we allow each individual data point $(x_i,y_i)$ to be described by\n",
    "a mixture between a Gaussian and a Lorentzian error model: \n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "p(\\{y_i\\}~|~\\theta,\\{g_i\\},\\sigma) = & g_i \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left[\\frac{-\\left(y_M(x_i~|~\\theta) - y_i\\right)^2}{2\\sigma^2}\\right] \\\\\n",
    "&+ (1 - g_i) \\frac{1}{\\sqrt{2}\\pi \\sigma} \\frac{1}{1 + \\left[ y_M (x_i~|~\\theta) - y_i\\right]^2 / (2\\sigma^2)}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "What we've done is expanded our model with some nuisance parameters, $\\{g_i\\}$, that is a series of weights which range from 0 to 1 and encode for each point $i$ the degree to which it fits the different error models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "643175fe0d0c97851ca8920313ec2f16",
     "grade": false,
     "grade_id": "cell-a78310b301c0142b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our model is much more complicated now: it has one new parameter per data point, but these can be considered nuisance parameters that can be marginalized-out in the end.  You can use a uniform prior $U(0,1)$ for these parameters. For the theoretical model we would recommend a Gaussian prior (mean=0, standard deviation $= \\sigma = 200$ km/sec) for the intercept, and a symmetric prior\n",
    "$$\n",
    "p(m|I) \\propto \\frac{1}{(1+m^2)^{3/2}}\n",
    "$$\n",
    "for the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b4ebda1ef32d7ee05fcd20910b86b42",
     "grade": false,
     "grade_id": "cell-b90416dd7ee7e48f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Your task is to implement this likelihood and to use, e.g., the [emcee](http://dan.iel.fm/emcee/current/) MCMC package to explore the parameter space.\n",
    "- Display the joint posterior for the slope and the intercept using a corner plot (i.e. marginalize over the g-parameters). \n",
    "- Extract the median and 68% credibility region for the slope parameter using the marginalized distribution.\n",
    "- Finally, study the posterior distribution for the g-parameters. Can you conclude which data points are more likely to be outliers (i.e. underestimated tails of the error distribution)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "848be4e8a3ca310d595cd288ee63690d",
     "grade": false,
     "grade_id": "cell-3b46910823ef4bfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the log prior, likelihood, posterior\n",
    "#\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ac7c5f34c0496db1dfce0ab8f1987abb",
     "grade": false,
     "grade_id": "cell-20bdcc85ad677753",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform the MCMC sampling\n",
    "#\n",
    "# Note that this step can be computationally costly. For the submitted version you are encouraged \n",
    "# to use less than 10,000 samples for each walker (and no more than 50 walkers)\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35fb17509d0da67e9c5ac2cee61e4fed",
     "grade": false,
     "grade_id": "cell-ca9936feec1cd0c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Corner plot for the intercept and slope parameters\n",
    "#\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59e0bd3d0959d9c74de7633af912fdd5",
     "grade": false,
     "grade_id": "cell-16b97e58dead861d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the posterior joint pdf for the g-parameters (using a corner plot)\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da76ed20beb88f64e5c480806e102e30",
     "grade": false,
     "grade_id": "cell-0b4a1547860a8776",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Which data points are most likely outliers?\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e70780d689f79a366aad104f7fde980e",
     "grade": false,
     "grade_id": "cell-014c22d2a511bfb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Task (b): Error propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f852636b01bcc86e0ff17d5cfd9fd96",
     "grade": false,
     "grade_id": "cell-a0f033cdf5cbc301",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The Bayesian approach offers a straight-forward approach for dealing with (known) systematic uncertainties; namely by marginalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "30462e2181bf64c49d10208b53a7168d",
     "grade": false,
     "grade_id": "cell-4b2fc9a3073b4407",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "The Hubble constant acts as a galactic ruler as it is used to measure astronomical distances according to $v = H_0 x$. An error in this ruler will therefore correspond to a systematic uncertainty in such measurements.\n",
    "\n",
    "Suppose that a particular galaxy has a measured recessional velocity $v_\\mathrm{measured} = (100 \\pm 5) \\times 10^3$ km/sec. Also assume that the Hubble constant $H_0$ is known from the analysis performed above in Step 3. Determine the posterior pdf for the distance to the galaxy assuming:\n",
    "1. A fixed value of $H_0$ corresponding to the posterior mean of the previous analysis.\n",
    "1. Using the full, sampled posterior pdf for $H_0$ from the same analysis.\n",
    "\n",
    "In this analysis we will set the intercept $m=0$ so that $v_\\mathrm{theory} = H_0 x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f61a4004c2bbb96fdac97d6bb53df902",
     "grade": false,
     "grade_id": "cell-69d116e04511ac3d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vm=100000\n",
    "sig_vm=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff35d4af2ca596f8372f852e183dd571",
     "grade": false,
     "grade_id": "cell-32466afe3f62abc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We assume that we can write\n",
    "$$\n",
    "v_\\mathrm{measured} = v_\\mathrm{theory} + \\delta v_\\mathrm{exp},\n",
    "$$\n",
    "where $v_\\mathrm{theory}$ is the recessional velocity according to our model, and $\\delta v_\\mathrm{exp}$ represents the noise component of the measurement. We that $\\delta v_\\mathrm{exp}$ can be described by a Gaussian pdf with mean 0 and standard deviation $\\sigma_v = 5 \\times 10^3$ km/sec. Note that we have also assumed that our model is perfect, i.e. $\\delta v_\\mathrm{theory}$ is negligible.\n",
    "\n",
    "In the following, we also assume that the error in the measurement in $v$ is uncorrelated with the uncertainty in $H_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2974dc3d790cceea71f078c6b339aed1",
     "grade": false,
     "grade_id": "cell-d8c05a2ee0b9f59a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Through application of Bayes' rule we can readily evaluate the posterior pdf $p(x|D,I)$ for the distance $x$ to the galaxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Case 1: Fixed $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b75772399f9666f552c3e7cbffdb0b72",
     "grade": false,
     "grade_id": "cell-110f0e736335617e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\\begin{align}\n",
    "p(x | D,I) & \\propto p(D | x, I) p(x|I) \\\\\n",
    "& = \\frac{1}{\\sqrt{2\\pi}\\sigma_v} \\exp \\left( - \\frac{(v_\\mathrm{measured} - v_\\mathrm{theory})^2}{2\\sigma_v^2} \\right) p(x|I)\\\\\n",
    "&= \\left\\{ \\begin{array}{ll} \\frac{1}{\\sqrt{2\\pi}\\sigma_v} \\exp \\left( - \\frac{(v_\\mathrm{measured} - H_0 x)^2}{2\\sigma_v^2} \\right) & \\text{with }x \\in [x_\\mathrm{min},x_\\mathrm{max}] \\\\\n",
    "0 & \\text{otherwise},\n",
    "\\end{array} \\right.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1b4b7dea8707ae8e420674b094874f8",
     "grade": false,
     "grade_id": "cell-2c8d4d2307ccaab1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "where $p(x|I)$ is the prior for the distance which we have assumed to be uniform, i.e. $p(x|I) \\propto 1$ in some (possibly large) region $[x_\\mathrm{min},x_\\mathrm{max}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86863627793d90af4fea423651c736b9",
     "grade": false,
     "grade_id": "cell-36ca7afdafe291ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the median value for H\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b9bfb63ad91d1d51b26cb50f17ce108",
     "grade": false,
     "grade_id": "cell-08dd8ada9d3be469",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Case 2: Using the inferred pdf for $H_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af8e9ecedcbf9250028f8d0fbfef9e2e",
     "grade": false,
     "grade_id": "cell-68267659d6b7f833",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here we use marginalization to obtain the desired posterior pdf $p(x|D,I)$ from the joint distribution of $p(x,H_0|D,I)$\n",
    "$$\n",
    "p(x|D,I) = \\int_{-\\infty}^\\infty dH_0 p(x,H_0|D,I).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "659b11236379014f5b8cb5995bf66f10",
     "grade": false,
     "grade_id": "cell-1cfd7011d138f04b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using Bayes' rule, the product rule, and the fact that $H_0$ is independent of $x$ we find that\n",
    "$$\n",
    "p(x|D,I) \\propto p(x|I) \\int dH_0 p(H_0|I) p(D|x,H_0,I),\n",
    "$$\n",
    "which means that we have expressed the quantity that we want (the posterior for $x$) in terms of quantities that we know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcf03adcd509ad320930da97733ec0e3",
     "grade": false,
     "grade_id": "cell-fd958a837942ce70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The pdf $p(H_0 | I)$ is known via its $N$ samples $\\{H_{i}\\}$ generated by the MCMC sampler.\n",
    "\n",
    "This means that we can approximate \n",
    "$$\n",
    "p(x |D,I) = \\int dH_0 p(H_0|I) p(D|x,H_0,I) \\approx \\frac{1}{N} \\sum_{i=1}^N p(D | x, H_i, I)\n",
    "$$\n",
    "where we have used $p(x|I) \\propto 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca17be79077b93e57d796810347f4c4e",
     "grade": false,
     "grade_id": "cell-bed19c575616409e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Infer the distance using the two different approaches (point estimate for H_0, and using the full pdf for H_0)\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
