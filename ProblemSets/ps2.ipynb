{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "251105c400969a6d24e9f7ec4883888e",
     "grade": false,
     "grade_id": "cell-6f99a2583f9fb27d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By changing the below boolean variable `student_self_assessment` to `True` you attest that:\n",
    "- All handed in solutions were produced by yourself in the sense that you understand your solutions and should be able to explain and discuss them with a peer or with a teacher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64a7020fd0e62e5b51ef4470ae1c797f",
     "grade": false,
     "grade_id": "student-self-assessment",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n",
    "student_self_assessment = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d873afed15d2d3de2ef460d53fccf90f",
     "grade": true,
     "grade_id": "cell-795bedd2908899aa",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert student_self_assessment == True, 'You must assert the individual solution statements.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2\n",
    "\n",
    "Last revised: 11-Sep-2023 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c532e9c42f0ac2234f7eb5f88c73672b",
     "grade": false,
     "grade_id": "cell-f3768fbcb502fd03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Data files are stored in\n",
    "DATA_DIR = \"DataFiles/\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 1: Standard Medical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Applying Bayesian rules of probability to a standard medical example\n",
    "Suppose there is an unknown disease (call it UD) that does not give any symptoms in its early phase, but that there is a test for it.\n",
    "\n",
    "a. The false positive rate is 2.3%. (\"False positive\" means the test says you have UD, but you don't.) <br>\n",
    "b. The false negative rate is 1.4%. (\"False negative\" means you have UD, but the test says you don't.)\n",
    "\n",
    "Assume that 1 in 10,000 people have the disease. You are given the test and get a positive result.  Your ultimate goal is to find the probability that you actually have the disease. \n",
    "$% Some LaTeX definitions we'll use.\n",
    "\\newcommand{\\pr}{\\textrm{p}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do it using the Bayesian rules.\n",
    "\n",
    "We'll use the notation:\n",
    "\n",
    "* $H$ = \"you have UD\"\n",
    "* $\\overline H$ = \"you do not have UD\"  \n",
    "* $D$ = \"you test positive for UD\"\n",
    "* $\\overline D$ = \"you test negative for UD\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks** \n",
    "\n",
    "Answer the following questions. Some of them are repeated on Yata.\n",
    "\n",
    "a. *Before doing a calculation (or thinking too hard :), does your intuition tell you the probability you have the disease is high or low?*\n",
    "<br>\n",
    "\n",
    "b. *In the $p(\\cdot | \\cdot)$ notation, what is your ultimate goal?*\n",
    "<br>\n",
    "\n",
    "c. *Express the false positive rate in $p(\\cdot | \\cdot)$ notation.* \\[Ask yourself first: what is to the left of the bar?\\]\n",
    "<br>\n",
    "\n",
    "d. *Express the false negative rate in $p(\\cdot | \\cdot)$ notation. By applying the sum rule, what do you also know? (If you get stuck answering the question, do the next part first.)* \n",
    "<br>\n",
    "\n",
    "e. *Should $p(D|H) + p(D|\\overline H) = 1$?\n",
    "    Should $p(D|H) + p(\\overline D |H) = 1$?\n",
    "    (Hint: does the sum rule apply on the left or right of the $|$?)*\n",
    "<br>\n",
    "\n",
    "f. *Apply Bayes' theorem to your result for your ultimate goal (don't put in numbers yet).\n",
    "   What other probabilities do we need?*\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Applying Bayesian rules of probability to a standard medical example (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill the probabilities as values for the \n",
    "# corresponding keys in the following dictionary.\n",
    "medical_example_probabilities = {}\n",
    "medical_example_probabilities['p(D|Hbar)'] = 0.0\n",
    "medical_example_probabilities['p(Dbar|H)'] = 0.0\n",
    "medical_example_probabilities['p(D|H)'] = 0.0\n",
    "medical_example_probabilities['p(H,Hbar|D)'] = 0.0\n",
    "medical_example_probabilities['p(Hbar)'] = 0.0\n",
    "medical_example_probabilities['p(D)'] = 0.0\n",
    "medical_example_probabilities['p(H|D)'] = 0.0\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp(D|Hbar)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp(Dbar|H)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp(D|H)\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m medical_example_probabilities[key] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m medical_example_probabilities[key] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m medical_example_probabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp(H,Hbar|D)\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key in ['p(D|Hbar)', 'p(Dbar|H)', 'p(D|H)']:\n",
    "    assert medical_example_probabilities[key] > 0.\n",
    "    assert medical_example_probabilities[key] < 1.\n",
    "    \n",
    "assert medical_example_probabilities['p(H,Hbar|D)'] <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp(Hbar)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp(D)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp(H|D)\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m medical_example_probabilities[key] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m medical_example_probabilities[key] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for key in ['p(Hbar)', 'p(D)', 'p(H|D)']:\n",
    "    assert medical_example_probabilities[key] > 0.\n",
    "    assert medical_example_probabilities[key] < 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 2: Coin tossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Define a function `bayesian_analysis_coin_flips` (see start code below) that returns the mean, median, and 68%/95% credible intervals of the Bayesian posterior with an input data array of coin flips and using a uniform $[0,1]$ prior for `pH` (probability of heads)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (mainly for testing your code)** Read the data with simulated coin tosses from the file `cointosses.dat`.\n",
    "Each row corresponds to a single toss: 0=tails; 1=heads\n",
    "\n",
    "Extract the mean and the 95% credible intervals (Degree-of-belief or DoB intervals) from the first 8 tosses, the first 64 tosses, the first 512 tosses and all 4096 tosses in the data assuming a uniform prior for the probability $p_H$ of obtaining heads in a single toss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint*: Sample code for computing the DoB interval is available in the demonstration notebook `demo-BayesianBasics.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f5396eeb530ca64889f4e7444703265",
     "grade": false,
     "grade_id": "cell-f27c4016e4570c15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd34410ae6a9662eac24de21527e4929",
     "grade": false,
     "grade_id": "cell-71d9ceb95f1f80db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = np.loadtxt(f'{DATA_DIR}/PS2_Prob1_cointosses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cc3e4665a66c30f9b147ce475a5973e",
     "grade": false,
     "grade_id": "cell-79bec52ebb8e7d12",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional. \n",
    "# Insert utility code / functions here.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5c76c6a0fb8a7e204839fbc79d600aa",
     "grade": false,
     "grade_id": "cell-538f840bea75b7dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that returns the mean, median, and 68%/95% credible intervals \n",
    "# of the Bayesian posterior with an input data array of coin flips \n",
    "# and using a uniform [0,1] prior for the pH (probability of heads).\n",
    "\n",
    "def bayesian_analysis_coin_flips(data_coin_tosses):\n",
    "    \"\"\"\n",
    "    Returns various Bayesian analysis results for the given data of coin tosses.\n",
    "    \n",
    "    The posterior is p( pH | data, I).\n",
    "    Assume a uniform p(pH|I) = U[0,1] prior\n",
    "    \n",
    "    Args:\n",
    "        data_coin_tosses: Array of shape (m,) with 'm' independent binary data.\n",
    "            0 = tails; 1 = heads\n",
    "            \n",
    "    Returns:\n",
    "        (mean, mode, median, dob68, dob95): A tuple with the following elements\n",
    "            mean: The mean of the posterior distribution (float)\n",
    "            mode: The mode of the posterior distribution (float)\n",
    "            median: The median of the posterior distribution (float)\n",
    "            dob68: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                68% degree-of-belief range of the posterior distribution (float,float)\n",
    "            dob95: A tuple (lo,hi) with the lower and upper limits of the \n",
    "                95% degree-of-belief range of the posterior distribution (float,float)\n",
    "    \"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b554d6660047c1cfc909c52ac6357d5",
     "grade": true,
     "grade_id": "cell-a567356cf18d3da5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m (mean, mode, median, dob68, dob95) \u001b[38;5;241m=\u001b[39m bayesian_analysis_coin_flips(data[:\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m (mean, mode, median, dob68[\u001b[38;5;241m0\u001b[39m], dob95[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrong type\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:1])\n",
    "for output in (mean, mode, median, dob68[0], dob95[0]):\n",
    "    assert output.dtype=='float64', 'Wrong type'\n",
    "assert len(dob68)==2, 'DoB tuple should be of length 2'\n",
    "assert len(dob95)==2, 'DoB tuple should be of length 2'\n",
    "assert np.abs(mean-0.667)<0.001\n",
    "assert np.abs(mode-1.0)<0.001\n",
    "assert np.abs(median-0.707)<0.001\n",
    "\n",
    "# 8 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:8])\n",
    "assert np.abs(mode-0.3750)<0.001\n",
    "assert np.abs(dob68[0]-0.2469)<0.001\n",
    "assert np.abs(dob68[1]-0.5538)<0.001\n",
    "# 1024 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:1024])\n",
    "assert np.abs(median-0.4922)<0.001\n",
    "# 4096 tosses\n",
    "(mean, mode, median, dob68, dob95) = bayesian_analysis_coin_flips(data[:4096])\n",
    "assert np.abs(dob95[0]-0.4727)<0.001\n",
    "assert np.abs(dob95[1]-0.5033)<0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 3: Bayesian linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fitting a linear model (in this case a first order polynomial) to a set data. Our model has two parameters $\\vec\\theta=[\\theta_0,\\theta_1]$ (pay attention to the indexing)\n",
    "\n",
    "$$\n",
    "y_M(x) = \\theta_0 + \\theta_1 x \n",
    "$$\n",
    "\n",
    "And our statistical model assumes that errors are independent and identically distributed\n",
    "\n",
    "$$\n",
    "y_i = y_M(x_i;\\theta) + \\varepsilon_i.\n",
    "$$\n",
    "\n",
    "Specifically, $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ and we assume a fixed standard deviation $\\sigma = 40$. \n",
    "\n",
    "(Note that the $\\varepsilon_i \\sim \\ldots$ notation means that $\\varepsilon_i$ is a draw of a random variable that follows the specified distribution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is generated from a straight line with intercept = 15. and slope = 1.5 plus random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = 15.\n",
    "slope = 1.5\n",
    "theta_true = np.array([intercept, slope])\n",
    "sigma=40."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from the file `PS2_Prob2_data.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dd517c45e4d41c20b1926e35aec8596",
     "grade": false,
     "grade_id": "cell-0609945db7cac2cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data and plot with fixed error bar \n",
    "# Use np.loadtxt() for loading data (the argument 'unpack=True' is useful)\n",
    "# and plt.errorbar() for plotting data with errorbars\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Bayesian linear regression: Implement log prior(s) and likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** Create functions that return the natural logarithm of the likelihood and two different choices of prior. Since we are seeking a parameter posterior you do not have to use proper normalization of the probability densities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two different choices for the prior:\n",
    "1. A uniform prior:\n",
    "   $$\n",
    "   p(\\theta_0, \\theta_1 | I) \\propto \\left\\{ \\begin{array}{ll} 1 & \\text{if } -100 \\le \\theta_0 \\le 100 \\text{ and } -100 \\le \\theta_1 \\le 100 \\\\ 0 & \\text{else} \\end{array}\\right.\n",
    "   $$\n",
    "2. A uniform prior for the intercept and a symmetry-invariant one for the slope\n",
    "   $$\n",
    "   p(\\theta_0, \\theta_1 | I) \\propto \\left\\{ \\begin{array}{ll} \\frac{1}{(1+\\theta_1^2)^{3/2}} & \\text{if } -100 \\le \\theta_0 \\le 100  \\\\ 0 & \\text{else}\\end{array}\\right.\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4809fccf1e4e2cea8aa848b42f8b2f34",
     "grade": false,
     "grade_id": "cell-85c54c4a189fc811",
     "locked": true,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def log_flat_prior(theta):\n",
    "    '''\n",
    "    Returns the log uniform prior (-100 <= theta_i <= 100)\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        \n",
    "    Returns:\n",
    "        logPi: (float) log prior (not necessarily normalized)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "def log_symmetric_prior(theta):\n",
    "    '''\n",
    "    Returns the log uniform (for theta_0) and symmetric (for theta_1) prior \n",
    "    \n",
    "    (-100 <= theta_0 <= 100; p(theta_1) \\propto (1+theta_1^2)^(-3/2))\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        \n",
    "    Returns:\n",
    "        logPi: (float) log prior (not necessarily normalized)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(log_flat_prior([0.,0.]), (np.floating, float)), 'The output should be a float'\n",
    "assert isinstance(log_symmetric_prior([0.,0.]), (np.floating, float)), 'The output should be a float'\n",
    "assert log_flat_prior([0.,0.]) - log_flat_prior([10.,-10.]) == 0., 'The flat prior should be constant in the interval'\n",
    "assert np.abs(log_symmetric_prior([-10.,1.]) - log_symmetric_prior([20.,2.]) - 1.3744360978112324) <0.001, \\\n",
    "'The log symmetric prior does not evaluate correctly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd0107dd227ac58207c31983b72d42ca",
     "grade": false,
     "grade_id": "cell-958720f73ee427a3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, dy=sigma):\n",
    "    '''\n",
    "    Returns the log likelihood.\n",
    "    \n",
    "    Args:\n",
    "        theta: array of floats with two elements. theta[0]=intercept. theta[1]=slope\n",
    "        x: data (independent variable). array of floats\n",
    "        y: data (dependent variable). array of floats\n",
    "        dy: fixed error (optional; default=sigma defined above), standard deviation of a normal distribution\n",
    "        \n",
    "    Returns:\n",
    "        logL: (float) log likelihood\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test = np.loadtxt(f'{DATA_DIR}/PS2_Prob2_data.txt',unpack=True)\n",
    "assert isinstance(log_likelihood([0,0], x_test, y_test), (np.floating, float)), 'The output should be a float'\n",
    "assert np.abs(log_likelihood([0.,0.], x_test, y_test) - log_likelihood([10.,1.], x_test, y_test) + 16.588440096874997) <0.001, \\\n",
    "'The log likelihood does not evaluate correctly.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Plot the posterior for the two different prior choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**\n",
    "* Where is the mode of the posterior with these two different priors?\n",
    "* Plot the joint pdf for the two different prior choices.\n",
    "* Are the parameters correlated / anti-correlated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24718fe14886b3b1a2920c8961d69f1c",
     "grade": false,
     "grade_id": "cell-ba8d7bc36ae04469",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We'll start by defining a function which takes a two-dimensional grid of likelihoods and \n",
    "# returns 1, 2, and 3-sigma contours. This acts by sorting and normalizing the values and then \n",
    "# finding the locations of the  0.682 ,  0.952 , and  0.9972  cutoffs:\n",
    "def contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D posterior\"\"\"\n",
    "    _sorted = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(_sorted) / np.sum(_sorted)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(_sorted[cutoffs])\n",
    "\n",
    "# Optional. \n",
    "# Insert utility code / functions here.\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c594ff32b6a8323cc8ff26774f5f64f",
     "grade": false,
     "grade_id": "cell-97243b6cddebf189",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The dictionary MAP (= maximum a posteriori) should return the mode \n",
    "# of the posterior distribution.\n",
    "# The key is the prior and the value is resulting posterior mode (peak)\n",
    "# given as theta* = [theta0*, theta1*]\n",
    "MAP={}\n",
    "MAP['uniform_prior'] = [0.0, 0.0]\n",
    "MAP['symmetric_prior'] = [0.0, 0.0]\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.abs(MAP[\"uniform_prior\"][1] - 1.977) < 0.01\n",
    "assert np.abs(MAP[\"symmetric_prior\"][0] - 18.725) < 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e1545461ec1e730d8f98e73236d2de5",
     "grade": false,
     "grade_id": "cell-9e84255bf61c598d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "- Plot the joint posterior for the two model parameters for the two different priors.\n",
    "- Indicate whether the slope and the intercept are correlated or anti-correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb5168adf85842656189bc7d12d07d58",
     "grade": false,
     "grade_id": "cell-80f2e16a2e4b7897",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 4: MCMC sampling of a Lorentzian pdf using the random walk Metropolis algorithm\n",
    "Note that you must solve this problem if you want to solve (extra) problem 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we have some function that tells us the (possibly unnormalized) probability density for a given position in a one-dimensional space. That is, the function is proportional to a univariate pdf. In general we will not be restricted to univariate distributions, but a key feature of the approach that we will implement here is that it can be straightforwardly extended to many dimensions. \n",
    "\n",
    "In this task we will assume a known, specific form of this univariate pdf, namely a Lorentzian (Cauchy) distribution, but it might just as well be some very complicated function that can only be evaluated numerically. All that is needed is some function that, for each position in the parameter space, returns a number that is proportial to the probability density.\n",
    "\n",
    "Let us start by studying the pdf that we will be sampling from using a random walk (that we will set up using the Metropolis algorithm outlined below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f02d7c456b144508548a5229c58cc78",
     "grade": false,
     "grade_id": "cell-39860bc3703e35b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Modules needed for this exercise\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import cauchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a number of random samples from the standard Cauchy\n",
    "r = cauchy.rvs(size=1000)\n",
    "plt.hist(r, density=True, histtype='stepfilled', alpha=0.2, \n",
    "         range=(-10,10),bins=21);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This histogram corresponds to a finite sample from the pdf of a standard Cauchy (Lorentzian)\n",
    "$$ \n",
    "p(x | \\alpha=0, \\beta=1) = \\frac{1}{\\pi(1+x^2)}, \n",
    "$$\n",
    "with its mode and median at $\\alpha=0$ and a full-width at half maximum (FWHM) equal to $2\\beta = 2$.\n",
    "\n",
    "Questions to ponder (not part of this problem; answers not requested):\n",
    "- How does this pdf compare with a standard normal distribution $\\mathcal{N}(x;\\mu=0,\\sigma^2=1)$?\n",
    "- The Cauchy distribution is often used in statistics as the canonical example of a \"pathological\" distribution since both its mean value and its variance are undefined. Do you see mathematically why these moments are undefined?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Construct a Metropolis sampler for univariate distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** First, turn the posterior into a callable function. You should deliberately remove the normalization to make the point that sampling can be made for an unnormalized pdf. Note that we will work directly with the pdf here (not taking the log as in previous examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7df526b99cc5c0235203a4a19de3c312",
     "grade": false,
     "grade_id": "cell-16bc7d26f9be4be2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def posterior_function(x, normalized=False):\n",
    "    '''\n",
    "    Return the posterior pdf given by a standard Cauchy (Lorentzian).\n",
    "    \n",
    "    Args:\n",
    "        x: position in a one-dimensional space\n",
    "        normalized: Return a normalized pdf if True (optional, default=False)\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task (see also Yata)** Now on to constructing the sampler. The code for a MCMC sampler that uses the Metropolis algorithm is enclosed below. However, it misses a few critical ingredients and your task is to add these at the correct places.\n",
    "\n",
    "1. At first, you have to specify the initial parameter position (that can be randomly chosen), lets fix it to the input argument `start_position`:\n",
    "\n",
    "```python\n",
    "current_position = start_position\n",
    "```\n",
    "\n",
    "Then, you propose to move (jump) to another position. You can be very dumb or very sophisticated about how you come up with the proposed jump. The Metropolis sampler is rather dumb and just picks a sample from a symmetric proposal distribution (here we again choose a normal distribution) centered around your current position (i.e. `current_position`) with a certain standard deviation (`proposal_width`) that will determine the length scale for proposed jumps \n",
    "\n",
    "2. Use `scipy.stats.norm` to create `proposed_position` as a sample from a random variable that is described by a normal distribution with the mean value equal to the `current_position` and a standard deviation that is given by `proposal_width`.\n",
    "\n",
    "Next, you evaluate whether that new position is a good place to jump to, or not. We quantify this by comparing the probability density at the `proposed_position` with the one for the `current_position`. Usually you would use the logarithms of the probability densities but we omit this here.\n",
    "\n",
    "3. Compute both `p_current` and `p_proposal`.\n",
    "\n",
    "Up until now, we essentially have a hill-climbing algorithm that would propose moves in random directions and only accept the step if the `proposed_position` has higher probability density than `current_position`. Eventually we'll get to the mode (at `x = 0` in this case) from where no more moves will be accepted. However, we want to sample a pdf so we'll also have to sometimes accept moves into regions of lower probability. The key trick is to compute the ratio of the two probabilities,\n",
    "\n",
    "```python\n",
    "p_accept = p_proposal / p_current\n",
    "```\n",
    "\n",
    "and to interpret this ratio as an acceptance probability. Note that the acceptance probability is obtained by dividing the pdf of the proposed parameter setting by the pdf of the current parameter setting. This implies that the probability density does not necessarily need to be normalized, the normalization factor will anyway cancel out. \n",
    "\n",
    "You can see that if `p_proposal` is larger, the acceptance probability will be `> 1` and we'll definitely accept the jump. However, if `p_current` is larger, say twice as large, there'll just be a 50% chance of moving to the proposed position. The acceptance of a proposed step will be decided by sampling another random number.\n",
    "\n",
    "4. Incorporate the acceptance step by comparing `p_accept` to a random number (uniform [0,1]). The `current_position` should be updated if the `accept` variable is `True`. \n",
    "\n",
    "Note that the `current_position` is added to our list of parameter samples at the end of the iteration, regardless of it being a new position or not.\n",
    "\n",
    "This simple procedure gives us samples from the pdf.\n",
    "\n",
    "The code below also prints detailed information if the optional keyword argument `verbose=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e881572709e4f7997afb5858e156d8a",
     "grade": false,
     "grade_id": "cell-30d4e414d6f57b40",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sampler(posterior_func, no_of_samples=4, start_position=.5, \n",
    "            proposal_width=1., verbose=False):\n",
    "    '''\n",
    "    Simple (but incomplete) Metropolis sampler function.\n",
    "    \n",
    "    Args:\n",
    "        posterior_func: Function that takes a single positional argument and returns \n",
    "            the (possibly unnormalized) pdf value.\n",
    "        no_of_samples: (integer) Number of samples that will be returned (excluding the start position). \n",
    "            (default=4)\n",
    "        start_position: (float) Start position. (default=0.5)\n",
    "        proposal_width: (float) Width of Gaussian proposal distribution. (default=1.)\n",
    "        verbose: (Boolean) Verbose output (default=False)\n",
    "        \n",
    "    Returns:\n",
    "        samples: Array of floats. Length = no_of_samples+1\n",
    "    '''\n",
    "    # Verbose printing\n",
    "    if verbose:\n",
    "        assert no_of_samples < 11, \"Too many samples for printing\"\n",
    "        print(f'Step    Trace  Proposed  Met.ratio  Accept')\n",
    "        \n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "    samples = [current_position]\n",
    "    for i in range(no_of_samples-1):\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "\n",
    "        # Compute posteriors of current and proposed position       \n",
    "        p_current = posterior_func(current_position)\n",
    "        p_proposal = posterior_func(proposed_position) \n",
    "        \n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "        \n",
    "        # Verbose\n",
    "        if verbose:\n",
    "            assert no_of_samples < 11, \"Too many samples for visualization\"\n",
    "            print(f'{i:>4}  {current_position:7.3f}   {proposed_position:7.3f}    {min(1.,p_accept):5.3f}     {accept}')\n",
    "        \n",
    "        # Possibly update position\n",
    "        # \n",
    "        # YOUR CODE HERE\n",
    "        # \n",
    "        \n",
    "        samples.append(current_position)\n",
    "        \n",
    "    return np.array(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell performs some first tests of the functionality of your sampler. It also makes a call to the sampler with the `verbose` option turned on. \n",
    "- How many proposed steps are accepted?\n",
    "- What can we say about the acceptance of proposed steps that would take us closer to the mode of the distribution?\n",
    "- And how about those that are proposed away from the mode?\n",
    "You can re-evaluate this cell multiple times to observe several chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae8f7262a929b1a18cb9d9c9b594aca8",
     "grade": true,
     "grade_id": "cell-2ae464bbbc58c95f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_samples = sampler(posterior_function, no_of_samples=100)\n",
    "assert test_samples[0]==0.5, 'The first position sample should be the default start position = 0.5'\n",
    "assert len(test_samples)==100, 'The sampler chain should contain the specified number of samples'\n",
    "\n",
    "sampler(posterior_function, start_position=0., no_of_samples=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70362bb4de477a566b0e0458661801ad",
     "grade": false,
     "grade_id": "cell-9a2da588961f37ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (b) Sampling the posterior\n",
    "**Task (see also Yata)** Draw 100,000 samples from your sampler and plot:\n",
    "* The trace (i.e. the sequence of draws of your single parameter x)\n",
    "* A normalized histogram of the samples compared to the true posterior pdf (normalized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a30bfa8e49901efaed73bb8fb68df4b",
     "grade": false,
     "grade_id": "cell-b7a2a59b1d878aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    samples = sampler(posterior_function, no_of_samples=100000, start_position=1.)\n",
    "except:\n",
    "    samples=None\n",
    "    print('The method \"sampler\" must be defined and working as expected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9bcca19ee39ddde1205424ae8a97988",
     "grade": false,
     "grade_id": "cell-17e4557f37667278",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plotting commands here\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "477b0214e8678bf92fd2f379a16b9f47",
     "grade": true,
     "grade_id": "cell-02cd9d30ab8b4c8c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# If the code works as expected your trace of samples should have a median value close to 0.0\n",
    "assert np.abs(np.median(samples))<0.2, f'The median of the samples is {np.median(samples):.1}. It should be close to the median of the posterior pdf (=0.0)'\n",
    "# Furthermore, the Cauchy distribution has heavy tails and your chain will surely have ventured out to rather large distances \n",
    "# (providing samples from the tails)\n",
    "assert np.abs(samples).max()>20, f'The furthest sample is {np.abs(samples).max():.1f}. Too small variations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 5: Signal and background\n",
    "This problem is more demanding than the other basic problems. Note that you must solve this problem if you want to solve (extra) problem 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this problem is to estimate the amplitude of a signal when there is a background.  We'll take a limiting case where the background is flat, so it is completely specified by its magnitude $B > 0$, and the signal is known to be a Gaussian with unknown amplitude $A$ but known position (mean) and width (standard deviation). \n",
    "\n",
    "The measurements will be integer numbers of counts $\\{N_k\\}$ in well-defined (equally spaced) bins $\\{x_k\\}$. The index $k$ runs over integers labeling the bins.\n",
    "\n",
    "We can imagine three different goals of the data analysis:\n",
    "- Finding $A$ and $B$ given $\\{N_k\\}$.\n",
    "- Finding $A$ (we do not care about $B$).\n",
    "- Finding $B$ (we do not care about $A$).\n",
    "\n",
    "In all cases we consider the bin sizes and the signal shape (including its mean position and width) as known information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our statistical model includes the true signal plus a constant background. The signal and the background magnitudes are the unknown parameters while the other parameters dictating the signal (width $w$ and mean $x_0$ of the Gaussian) are known and fixed:\n",
    "\n",
    "$$\n",
    "   D_k = n_0 \\left[ A e^{-(x_k-x_0)^2/2 w^2} + B \\right]\n",
    "$$\n",
    "\n",
    "Here $n_0$ is a constant that scales with measurement time.  Note that $D_k$ is not an integer in general, unlike $N_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51508b3c6f05fd3a57d7223ab8f6dd50",
     "grade": false,
     "grade_id": "cell-ade2557e6088a291",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# import statements.\n",
    "# We use pickle to save and load a python dictionary\n",
    "import pickle\n",
    "# factorial from the math module is useful. You might consider other modules as well.\n",
    "from math import factorial\n",
    "\n",
    "# import additional modules as needed\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "644664b1809f5a1557ece48c4a674325",
     "grade": false,
     "grade_id": "cell-cea19416f4d5f062",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function generates data according to the statistical model\n",
    "A_true = 1.\n",
    "B_true = 2.\n",
    "\n",
    "def exact_data(A, B, n_0, x_k, x_0=0., width=np.sqrt(5.)):\n",
    "    \"\"\"\n",
    "    Return the exact signal plus background.  The overall scale is n_0,\n",
    "    which is determined by how long counts are collected. \n",
    "    The default signal position and width are 0.0 and sqrt(5), respectively (in some  irrelevant units).\n",
    "    \"\"\"\n",
    "    return n_0 * (A * np.exp(-(x_k - x_0)**2/(2.*width**2)) + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poisson distribution\n",
    "We are imagining a counting experiment, so the statistics of the counts we record will follow a Poisson distribution. It might be an interesting exercise to derive why this distribution is expected for a counting experiment. \n",
    "\n",
    "The Poisson discrete random variable from scipy.stats is defined by (see [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html))\n",
    "\n",
    "$$\n",
    "p(k \\mid \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!} \\quad \\mbox{for }k\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $k$ is an integer and $\\mu$ is called the shape parameter. The mean and variance of this distribution are both equal to $\\mu$. Sivia and Gregory each use a different notation for for this distribution, which means you need to be flexible. \n",
    "\n",
    "For convenience, we'll define our own version in this notebook:\n",
    "\n",
    "$$\n",
    "p(N \\mid D) = \\frac{D^N e^{-D}}{N!} \\quad \\mbox{for }N\\geq 0 \\;.\n",
    "$$\n",
    "\n",
    "where $N$ is an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f44128714159ffde91418ff139a6f2b0",
     "grade": false,
     "grade_id": "cell-e10dd1c95952f91a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# make a dataset for exploring\n",
    "def make_dataset(A_true, B_true, width, x_0, databins=15, delta_x=1, D_max=100):\n",
    "    \"\"\"\n",
    "    Create a data set based on the number of bins (databins), the spacing\n",
    "    of bins (delta_x), and the maximum we want the exact result to have\n",
    "    (D_max, this fixes the n_0 parameter).\n",
    "    \n",
    "    Return arrays for the x points (xk_pts), the corresponding values of the\n",
    "    exact signal plus background in those bins (Dk_pts), the measured values\n",
    "    in those bins (Nk_pts, integers drawn from a Poisson distribution), the \n",
    "    maximum extent of the bins (x_max) and n_0.\n",
    "    \"\"\"\n",
    "    # set up evenly spaced bins, centered on x_0\n",
    "    x_max = x_0 + delta_x * (databins-1)/2\n",
    "    xk_pts = np.arange(-x_max, x_max + delta_x, delta_x, dtype=int)\n",
    "    \n",
    "    # scale n_0 so maximum of the \"true\" signal plus background is D_max\n",
    "    n_0 = D_max / (A_true + B_true)  \n",
    "    Dk_pts = exact_data(A_true, B_true, n_0, xk_pts, width=width)\n",
    "    \n",
    "    # sample for each k to determine the measured N_k\n",
    "    Nk_pts = [stats.poisson.rvs(mu=Dk) for Dk in Dk_pts]\n",
    "    \n",
    "    return xk_pts, Dk_pts, Nk_pts, x_max, n_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the signal and the data (these tasks are not graded but will help you to understand the problem)\n",
    "* Make a plot of the true signal plus background we are trying to deduce. Use $A_\\mathrm{true}=1$ and $B_\\mathrm{true}=2$ and the signal position (mean) $x_0=0$ and width (standard deviation)  $w=\\sqrt{5}$.\n",
    "\n",
    "We consider what happens for fixed signal and background but changing the experimental conditions specified by `D_max` and `databins` (we'll keep `delta_x` fixed to 1). In all cases the bins are symmetric around $x=0$.\n",
    "\n",
    "The pickle file that is loaded in the cell below contains data from four differently designed counting experiments.:\n",
    "1. Baseline case: 15 bins and maximum expection of 100 counts per bin.\n",
    "1. Low statistics case: 15 bins and maximum expection of only 10 counts per bin.\n",
    "1. Greater range case: 31 bins (with fixed bin width) and maximum expection of 50 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    "1. Smaller range case: 7 bins (with fixed bin width) and maximum expection of 200 counts per bin to give approximately the same total number of counts as in baseline case.\n",
    " \n",
    "* Make four subplots that correspond to the data from the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4145094cc5862704b16de29e927a54c",
     "grade": false,
     "grade_id": "cell-b300bc6fd3ee1f07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded \"data\" dictionary from file.\n",
      "Extract data with:\n",
      "xk_pts, Dk_pts, Nk_pts, x_max, n_0 = data[case]\n",
      "where the key \"case\" is one of:\n",
      "dict_keys(['Baseline', 'Low Statistics', 'Greater Range', 'Smaller Range'])\n"
     ]
    }
   ],
   "source": [
    "# Plotting the signal and background\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# \n",
    "import pickle\n",
    "\n",
    "# The data has been generated already and will be loaded from a pickle file.\n",
    "# It is a dictionary with four keys corresponding to the four cases, and each value\n",
    "# corresponding to xk_pts, Dk_pts, Nk_pts, x_max, n_0\n",
    "with open(f'{DATA_DIR}/PS2_Prob4_data.pickle','rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    print('Loaded \"data\" dictionary from file.')\n",
    "    print('Extract data with:')\n",
    "    print('xk_pts, Dk_pts, Nk_pts, x_max, n_0 = data[case]')\n",
    "    print('where the key \"case\" is one of:')\n",
    "    cases = data.keys()\n",
    "    print(cases)\n",
    "\n",
    "# Plotting the data for the four cases\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08a0ca5360d4bad5af09dd37963a9366",
     "grade": false,
     "grade_id": "cell-d1a0b4b6a44f1f7b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### (a) Compute and plot the joint posterior on a grid\n",
    "**Tasks (see also Yata)**\n",
    "* Implement functions for the (log) likelihood and for a uniform (log) prior. Let's use a uniform prior for $0 \\le A \\le 5$ and $0 \\le B \\le 5$.\n",
    "* Evaluate the log-posterior on a grid and then: \n",
    "  - Plot the joint posterior pdf for $A$ and $B$ for the four cases.\n",
    "  - Plot the marginalized posterior pdf for the signal amplitude $A$ for the four cases.\n",
    "  \n",
    "  Use the same axis scales for all four cases such that the precision of the inference can be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a1f8ba7ff4c390ee59ae590ca751f0a",
     "grade": false,
     "grade_id": "cell-a3772dfb9786e84d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Define the pdfs and combine with Bayes' theorem.\n",
    "\n",
    "def log_prior(A, B):\n",
    "    \"\"\"\n",
    "    Log prior .\n",
    "    \n",
    "    We take a uniform (flat) prior with large enough\n",
    "    maximums but, more importantly, require positive values for A and B.\n",
    "    \"\"\"\n",
    "    A_max = 5.\n",
    "    B_max = 5.\n",
    "    # flat prior \n",
    "    if np.logical_and(A <= A_max, B <= B_max).all(): \n",
    "        return np.log(1/(A_max * B_max))\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "\n",
    "def log_likelihood(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log likelihood for data Nk_pts given A and B\"\"\"\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "def log_posterior(A, B, xk_pts, Nk_pts, n_0):\n",
    "    \"\"\"Log posterior for data Nk_pts given A and B\"\"\"\n",
    "    return log_prior(A, B) + log_likelihood(A, B, xk_pts, Nk_pts, n_0)\n",
    "\n",
    "# Other utility code can be put here (if needed)\n",
    "#\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cb9360be5de8932f51c3531dcaaa9dd",
     "grade": false,
     "grade_id": "cell-cbb85be603215ec4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Optional.\n",
    "# Code to find contour levels of gridded 2D posterior.\n",
    "\n",
    "def find_contour_levels(grid):\n",
    "    \"\"\"Compute 1, 2, 3-sigma contour levels for a gridded 2D posterior\n",
    "       Note: taken from BayesianAstronomy but may not work here.\n",
    "    \"\"\"\n",
    "    sorted_ = np.sort(grid.ravel())[::-1]\n",
    "    pct = np.cumsum(sorted_) / np.sum(sorted_)\n",
    "    cutoffs = np.searchsorted(pct, np.array([0.68, 0.95, 0.997]) ** 2)\n",
    "    return np.sort(sorted_[cutoffs])\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "11f0ba8e4b6bee89a8c5268cdb67cc35",
     "grade": false,
     "grade_id": "cell-219fdc3158a3dc82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (b) Design of experiment\n",
    "**Tasks (see also Yata)** \n",
    "Answer the following questions. Some of them are repeated on Yata.\n",
    "  1. Can you understand why the signal and background amplitudes are anticorrelated? And why the (anti)correlation seems to be stronger in one of the cases? \n",
    "  1. What are your conclusions for how to design the experiment given limited resources? \n",
    "    - In particular, given that you wanted to be able to distinguish between signal amplitude and background, would it then be better to have many counts in few bins, or the same total amount of counts spread over a wider interval? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f047ae46ddc8ee3d71001c28128e322a",
     "grade": true,
     "grade_id": "cell-620bd3518d4f7363",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Problem 6: Assigning probabilities for a hundred-sided dice\n",
    "Consider a hundred-sided dice (labeled with 1, 2, 3, ..., 100) for which you know \n",
    "\n",
    "Case 1. that the mean of a large number of rolls is $\\mu_1 = \\frac{1}{100}\\sum_{i=1}^{100} i$.\n",
    "\n",
    "Case 2. that the mean of a large number of rolls is $\\mu_2=40$ and that the standard deviation is $\\sigma_2=25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Analytical MaxEnt derivation of the probability distributions\n",
    "**Task**\n",
    "* Use the principle of maximum entropy to assign the probabilities $\\{ p_i \\}_{i=1}^{100}$ for the outcomes of a dice roll in the two different cases. Employ the method of Lagrange multipliers to derive (analytical) expressions for $p_i$.\n",
    "* Your expressions should contain the (yet undetermined) Lagrange multipliers.\n",
    "* You should use the markdown cell below to perform the analytical derivation and present the results for $p_i$ in case 1 and case 2. Start by writing down the expression for the entropy with the relevant Lagrange-multiplier terms.\n",
    "\n",
    "*Hint: There are various constraints from the known information: the normalization of the probabilities $\\mathcal{N} = \\sum_i p_i = 1$ and the mean result $\\mu=\\sum_i i p_i$. In case 2 there is also a third constraint in the variance $\\sigma^2 = \\sum_i (i-\\mu)^2 p_i$. Set the so called Lebesque measure $m_i = 1 \\; \\forall i$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "**PLEASE WRITE YOUR ANSWER HERE** \n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Determination of the Lagrange multipliers\n",
    "**Tasks**\n",
    "* Determine the still unknown Lagrange multipliers; \n",
    "  - for case 1 they can be derived analytically, but it is also allowed to determine them numerically.\n",
    "  - for numerical determination you might find that it works better to use `scipy.fsolve` to find the root of $\\partial Q / \\partial p_i$ (where $Q$ is the constrained entropy) than to use `scipy.optimize` to find the maximum of $Q$.\n",
    "* Print the values of the Lagrange multipliers for each case.\n",
    "* Assign the probabilities and make a bar plot for each case. Store the probabilities in the arrays `probs_1` and `probs_2`, each of shape (100,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#...\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We suggest to define helper functions (but you are free to use your own solution)\n",
    "#\n",
    "def pdf_1(lam, M=100, return_norm=False):\n",
    "    '''\n",
    "    Returns an array of (normalized) probabilities for a given Lagrange multiplier. Case a.\n",
    "    \n",
    "    Args:\n",
    "        lam: Lagrange multiplier (float)\n",
    "        M: number of discrete probabilities (int). Default = 100\n",
    "        return_norm: Returns the normalization factor as a second variable (boolean). Default = False\n",
    "        \n",
    "    Returns:\n",
    "        pdf: Array of shape (M,) of probabilities p_i = f_1(i,lam) / norm\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def pdf_2(lam1, lam2, mu, M=100, return_norm=False):\n",
    "    '''\n",
    "    Returns an array of (normalized) probabilities for a given Lagrange multiplier and mean. Case b.\n",
    "    \n",
    "    Args:\n",
    "        lam1: Lagrange multiplier for the mean constraint (float)\n",
    "        lam2: Lagrange multiplier for the variance constraint (float)\n",
    "        mu: mean value (float)\n",
    "        M: number of discrete probabilities (int). Default = 100\n",
    "        return_norm: Returns the normalization factor as a second variable (boolean). Default = False\n",
    "        \n",
    "    Returns:\n",
    "        pdf: Array of shape (M,) of probabilities p_i = f_2(i,lam1,lam2,mu) / norm\n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "\n",
    "def moments(pdf):\n",
    "    '''\n",
    "    Returns the first few moments of a discrete pdf.\n",
    "    \n",
    "    Args:\n",
    "        pdf: Array of shape (M,) of probabilities p_i\n",
    "        \n",
    "    Returns:\n",
    "        moments: tuple of floats with the first few moments (norm, mean, variance) \n",
    "    '''\n",
    "    # \n",
    "    # YOUR CODE HERE\n",
    "    # \n",
    "    \n",
    "# and to use the principle of maximum entropy to assign the probabilities \n",
    "Msides = 100\n",
    "probs_1 = np.ones(Msides)\n",
    "probs_2 = np.ones(Msides)\n",
    "\n",
    "# Don't forget to also make a bar plot\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iprobs, probs in enumerate([probs_1,probs_2]):\n",
    "    assert probs.shape == (100,), f'The array `probs` for case {iprobs} should be of shape (100,). probs.shape = {probs.shape}'\n",
    "    assert np.abs(probs.sum()-1.0)<1e-6, f'The norm of array `probs` for case {iprobs} is {probs.sum}'\n",
    "\n",
    "iside = np.arange(100)+1\n",
    "pdf_means=np.array([0.,0.])\n",
    "pdf_variances=np.array([0.,0.])\n",
    "for iprobs, probs in enumerate([probs_1,probs_2]):\n",
    "    pdf_means[iprobs] = np.sum(probs*iside)\n",
    "    pdf_variances[iprobs] = np.sum((iside-pdf_means[iprobs])**2*probs)\n",
    "assert (np.abs(pdf_means-np.array([50.50,40]))<1e-4).all(), f'The mean values are: {pdf_means}'\n",
    "assert (np.abs(pdf_variances-np.array([833.25,625.]))<1e-3).all(), f'The variances are: {pdf_variances}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
