{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project I: Parameter estimation for a toy model of an effective field theory\n",
    "## Learning from data [TIF285], Chalmers, Fall 2022\n",
    "\n",
    "Last revised: 3-Sep-2022 by Christian Forss√©n [christian.forssen@chalmers.se]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See deadline on the course web page\n",
    "- This project is performed in groups of two students. \n",
    "- The second part of the project is optional. See examination rules on the course web page.\n",
    "- Hand-in your written report and your solution source code via Canvas.\n",
    "- Students are allowed to discuss together and help each other when working on the projects. However, every student must understand and be able to explain their submitted solutions. Plagiarism (of text and/or code) is not allowed (submissions will be both manually and automatically monitored)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written report\n",
    "- Page limit: 6 pages (excluding title page and list of references). 3 extra pages are allowed when doing also the optional extra task.\n",
    "- Give a short description of the nature of the problem and the methods you have used.\n",
    "- Include your results either in figure or tabular form. All tables and figures should have relevant captions and labels on the axes.\n",
    "- Try to give an interpretation of you results.\n",
    "- Upload the source code of your program as a separate file (.ipynb or .py). Comment your program properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main task\n",
    "The overall project goal is to reproduce various results in a paper: [*Bayesian parameter estimation for effective field theories*](https://arxiv.org/abs/1511.03618) by Sarah Wesolowski, et al.  It's a long paper, so don't try to read all of it!  We'll guide you to the important parts. Sec. II of the paper is the most relevant, but you don't need to understand all of it. \n",
    "\n",
    "The paper uses a toy model to simulate an effective field theory (EFT), namely a Taylor series of some specified function. A specific aim is to present guidelines for parameter estimation in the situation where you have some knowledge of the model that can be used in a Bayesian way. We will not discuss the EFT concept in any detail, but will just mention that it is a type of approximation, or effective theory, for a more fundamental (underlying) physical theory. An EFT starts from the relevant degrees of freedom to describe physical phenomena occurring at a chosen length scale or energy scale, while not resolving substructure and degrees of freedom at shorter distances (or, equivalently, at higher energies). \n",
    "\n",
    "In our case, the function\n",
    "$$\n",
    "g(x) = \\left(\\frac12 + \\tan\\left(\\frac{\\pi}{2}x\\right)\\right)^2\n",
    "$$\n",
    "represents the true, underlying theory.  Its Taylor series at $x=0$ is\n",
    "$$\n",
    "g(x) = 0.25 + 1.57 x + 2.47 x^2 + 1.29 x^3 + \\cdots\n",
    "$$\n",
    "where the expansion parameters are known in this toy example (since we know the underlying theory), but would not be in a real situation.\n",
    "\n",
    "Our \"effective theory\" then corresponds to the expansion\n",
    "$$\n",
    "g_{\\rm th}(x) \\equiv \\sum_{i=0}^k a_i x^i,\n",
    "$$\n",
    "where $a_i$ are the parameters. For an EFT, these are sometimes known as low-energy constants (LECs).\n",
    "\n",
    "Your general task is to fit the parameters $a_i$, up to some truncation order $k$, using a Bayesian approach, and analyze the results. \n",
    "\n",
    "* **Your specific goal is to reproduce and interpret Figure 1 on page 6 of the arXiv preprint. This figure shows joint pdfs for the EFT parameters using two different priors. One of these priors encapsulates the physics expectation that the parameters should be of \"natural\" size (order one).**\n",
    "* **A secondary goal is to reproduce Figs. 3 and 4 that show predictions of the inferred EFT with error bands. You can either use $k=k_\\mathrm{max}=3$ as in the first task, or $k=k_\\mathrm{max}=4$ as in the preprint.** \n",
    "\n",
    "You should use emcee to sample the joint pdfs and you should use corner to make plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning goals:\n",
    "* Apply and extend the Bayesian parameter estimation ideas and techniques from the course.\n",
    "* This a less-guided set of tasks and you will have to put together ideas and tools we've discussed.\n",
    "* Explore the impact of control features:  dependence on how much data is used and how precise it is; apply an *informative* prior.\n",
    "* Learn about some diagnostics for Bayesian parameter estimation.\n",
    "* Try out sampling on a controlled problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions for how to proceed:\n",
    "* Follow the lead of the exercise notebooks.\n",
    "* Load the data set that was used in the paper: [D1_c_5.dat](https://arxiv.org/src/1511.03618v3/anc/D1_c_5.dat). The three columns correspond respectively to $x_j$, the measured data $d(x_j) \\equiv d_j$ and the error in terms of a standard deviation $\\sigma_j$.\n",
    "* Define functions for the two choices of prior: a wide uniform prior, e.g. $|a_i|<100$, and a Gaussian naturalness prior given by Eq. (24) with $\\bar{a}=5$. Use the log prior.\n",
    "* Define a function for the likelihood (Eq. 9). It will require the chi-squared measure. Use the log likelihood.\n",
    "* Call emcee to sample the posteriors. \n",
    "* Use corner to create plots.  You can read the answers for the tables from the corner plots.\n",
    "* Don't try to do too much in your code at first (start with a low order). \n",
    "* Generate figures for the lowest orders analogous to Figure 1 and then reproduce Figure 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments and suggestions\n",
    "* The `show_titles=True` option to corner will show central results and one-$\\sigma$ error limits on the projected posterior plots.\n",
    "* The `quantiles=[0.16, 0.5, 0.84]`option to corner adds the dashed vertical lines to the marginal posteriors on the diagonal. You can obviously change the quantiles if you want another credibility region.\n",
    "* The python command `np.percentile(y, [16, 50, 84], axis=0)` might be useful to extract numerical values for the credibility region and the median from a python array `y`.\n",
    "* When reproducing Figures 3 and 4 you can use Matplotlib's `fill_between(x, y-error, y+error)` to make bands.  (Use the `alpha` keyword for `fill_between`, e.g., `alpha=0.5`, to make the bands more transparent.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Part 2 (extra task)\n",
    "* Reproduce and interpret Table III on page 12 of the arXiv preprint. \n",
    "* Repeat the analysis with the same function but different data precision and/or quantity (number of data points). You will then have to generate the data yourself using Eq. (2) and the true function Eq. (23). It is probably wise to stay in the $0 < x \\le 1/\\pi$ range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
